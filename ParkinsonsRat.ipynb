{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rgw9LHbgWkBO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.0\n",
      "Requirement already satisfied: snntorch in /opt/conda/lib/python3.11/site-packages (0.9.1)\n",
      "Requirement already satisfied: torch>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from snntorch) (2.3.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from snntorch) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from snntorch) (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from snntorch) (1.26.4)\n",
      "Requirement already satisfied: nir in /opt/conda/lib/python3.11/site-packages (from snntorch) (1.0.4)\n",
      "Requirement already satisfied: nirtorch in /opt/conda/lib/python3.11/site-packages (from snntorch) (1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.1.0->snntorch) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.1.0->snntorch) (12.5.40)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (4.52.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib->snntorch) (2.9.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (from nir->snntorch) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->snntorch) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->snntorch) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->snntorch) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.1.0->snntorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.1.0->snntorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install snntorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Rgw9LHbgWkBO"
   },
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import statistics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import math \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from snntorch import functional as SF\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SETTINGS: \n",
    "\n",
    "# All parameters\n",
    "num_inputs = 80 \n",
    "num_outputs = 2\n",
    "beta = .95 \n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "lr = 1e-3 \n",
    "lr_decay = True \n",
    "lr_decay_gamma = 0.9 \n",
    "# Dataset parameters\n",
    "# filepaths for training and testing data \n",
    "root = \"Datasets/rat_pd_sim_data\"\n",
    "training_data = \"TestDataSet_1024\"\n",
    "testing_data = \"VariedTrainingData\"\n",
    "num_steps = 150\n",
    "num_steps_eval = 400 \n",
    "\n",
    "\n",
    "# number of steps to hash spikes in data into. Should always be <= time steps of simulation data being loaded. \n",
    "# if smaller than simulation steps, data will be compressed into a smaller timeframe. This will speed up the model, but loses some timing granularity for neuron spikes. \n",
    "# if equal, no information is lost, and the model runs with exactly the same number of time steps as then rat simulation. \n",
    "\n",
    "# MOME parameters \n",
    "num_hidden = 144 # must be a perfects square. num_hidden = n^2 where n is an integer \n",
    "hidden_layers = 3 # This parameter is used ONLY for calculations, it will not effect network architecture. Should be set to correspond correctly to the networks actually created \n",
    "# keytopk^2 <= per_head_topk\n",
    "key_topk = 10\n",
    "per_head_topk = 64\n",
    "\n",
    "\n",
    "# Comparison network settings \n",
    "normalize_params = True \n",
    "normalize_neurons = not normalize_params\n",
    "\n",
    "# generate output folder: \n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "out_folder_name = f\"figs/run_at_{timestamp}\"\n",
    "if not os.path.exists(out_folder_name):\n",
    "    os.makedirs(out_folder_name)\n",
    "\n",
    "def write_params(filename, params_dict): \n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(params_dict, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(i, min, max):\n",
    "    return (i - min) / (max - min)\n",
    "\n",
    "# Hasher function used to compress spike times into our simulation time. Simple implimentation. \n",
    "# n: size of output array. Should equal to num_steps\n",
    "# arr: The array to be hashed\n",
    "# min/ max: Maximum and minimum values in input array. Max should be very slightly higher than the actually number, otherwise indexing errors can occur. \n",
    "# output: New hashed array of size n \n",
    "# Input is an array containing the 'time stamps' ie 1.01 for a spike occuring 1.01 seconds into simulation \n",
    "# This function than turns that spike into a '1' in an array of n indices, where i now becomes time \n",
    "def hasher(n, arr, min=0., max=2.0001):\n",
    "    if len(arr) > n:\n",
    "        print(\"There is too many items in this array to hash\")\n",
    "        print(f\"The num_steps is {n} and the array length is {len(arr)}\")\n",
    "        print(\"Inserting an array full of ones instead\")\n",
    "        return np.ones(n)\n",
    "\n",
    "    back_flag = False\n",
    "    out = np.zeros(n)\n",
    "    for item in arr: \n",
    "        norm = normalize(item, min, max)\n",
    "        #print(norm)\n",
    "        index = math.floor(norm * n)\n",
    "        while out[index] == 1:\n",
    "            if index == 0:\n",
    "                back_flag = False\n",
    "            if index == n-1:\n",
    "                    back_flag = True\n",
    "\n",
    "            if back_flag: \n",
    "                index -= 1 \n",
    "                if index == 0:\n",
    "                    back_flag = False\n",
    "            else:\n",
    "                index += 1 \n",
    "                if index == n-1:\n",
    "                    back_flag = True\n",
    "\n",
    "        back_flag = False \n",
    "        \n",
    "        out[index] = 1\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68u9aLvarM6b",
    "outputId": "5d28f31e-f4da-49eb-a499-c6fd737faaf5"
   },
   "outputs": [],
   "source": [
    "# unpack_hash: \n",
    "# data: input the contents of one brain region from mat file \n",
    "# output: the contents of that file, now in in a singular array instead of nested lists and arrays \n",
    "# hashed via hash function above \n",
    "\n",
    "def unpack_hash(data):\n",
    "    data = np.array(data[0])\n",
    "    unpacked = [hasher(num_steps, item[0].flatten()) for item in data] \n",
    "    arr = np.array(unpacked)\n",
    "\n",
    "    return np.fliplr(np.rot90(arr, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass list of matfiles and elem to find maxlen of that attribute \n",
    "def max_len(mat_list, elem):\n",
    "    max_elem = 0 \n",
    "    for mat in mat_list:\n",
    "        data = mat[elem]\n",
    "        temp = max([len(item[0]) for item in data[0]])\n",
    "        \n",
    "        if(max_elem < temp):\n",
    "            max_elem = temp \n",
    "\n",
    "    return max_elem\n",
    "\n",
    "#pass list of matfiles and optional integer 'wiggle' \n",
    "#first, each matfile is passed through max_len function for each attribute of concern \n",
    "#wiggle is added to end of calculated maxlen. This is to ensure no problems arrive if testing data has some elements longer than training \n",
    "# TODO: some way to pass calculated maxlens to testdata will be important, so vector size is consistent between training and testing sets \n",
    "# then each file in mat_list is unpacked, and appended to each other to create one vector for each matfile \n",
    "# list of input vectors returned \n",
    "\n",
    "def constructor_hash(mat_list):\n",
    "    out = []\n",
    "\n",
    "    for mat in mat_list:\n",
    "        TH_APs_data = unpack_hash(mat['TH_APs'])\n",
    "        STNAPs_data = unpack_hash(mat['STN_APs'])\n",
    "        GPe_APs_data = unpack_hash(mat['GPe_APs'])\n",
    "        GPi_APs_data = unpack_hash(mat['GPi_APs'])\n",
    "        Striat_APs_indr_data = unpack_hash(mat['Striat_APs_indr'])\n",
    "        Striat_APs_dr_data = unpack_hash(mat['Striat_APs_dr'])\n",
    "        Cor_APs_data = unpack_hash(mat['Cor_APs'])\n",
    "\n",
    "        build = np.append(TH_APs_data, STNAPs_data, axis=1)\n",
    "        build = np.append(build, GPe_APs_data, axis=1)\n",
    "        build = np.append(build, GPi_APs_data, axis=1)\n",
    "        build = np.append(build, Striat_APs_indr_data, axis=1)\n",
    "        build = np.append(build, Striat_APs_dr_data, axis=1)\n",
    "        build = np.append(build, Cor_APs_data, axis=1)\n",
    "        out = out + [build]\n",
    "        \n",
    "    return np.array(out)\n",
    "\n",
    "# pass dir = Directory containing samples \n",
    "# mat = name of the mat file you want to open \n",
    "def open_mat(dir, mat_name): \n",
    "    filename = os.path.join(dir, mat_name)\n",
    "    mat = loadmat(filename)\n",
    "    return mat\n",
    "\n",
    "# input: list of mat files\n",
    "# output: List of all gpi_alpha_beta_area's from mat files \n",
    "\n",
    "def get_beta_oscilation(mat_list):\n",
    "    out = [] \n",
    "    for mat in mat_list: \n",
    "        out = out + [mat[\"gpi_alpha_beta_area\"][0][0]]\n",
    "\n",
    "    return out \n",
    "\n",
    "# dir = directory to load (ie Training Validation Set)\n",
    "# os.loadir() loads all the filenames in a given directory\n",
    "def load_dir(dir): \n",
    "    out = []\n",
    "    bin_labels = [] \n",
    "    for mat_file in os.listdir(dir):\n",
    "        if 'con' in mat_file:\n",
    "            out = out + [open_mat(dir, mat_file)]\n",
    "            bin_labels = bin_labels + [0]\n",
    "        elif 'pd' in mat_file:\n",
    "            out = out + [open_mat(dir, mat_file)]\n",
    "            bin_labels = bin_labels + [1]\n",
    "    \n",
    "    alpha_beta = get_beta_oscilation(out) \n",
    "    out = constructor_hash(out)\n",
    "    labels = list(zip(alpha_beta, bin_labels))\n",
    "    \n",
    "    return out, labels  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PNVHxUb0cBXq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 150, 80])\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "# NOTE: programmed on device without cuda. If device issues arise, ensure all relevant items properly cast to device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# set batch size\n",
    "\n",
    "\n",
    "# Open directory specified, and cast build dataloader \n",
    "def build_dataloader(dir):\n",
    "    full_dir, labels = load_dir(dir)\n",
    "    data_tensor = torch.Tensor(full_dir).to(device)\n",
    "    print(data_tensor.size())\n",
    "    label_tensor = torch.Tensor(labels).to(device)\n",
    "    dataset = torch.utils.data.TensorDataset(data_tensor, label_tensor)\n",
    "    data = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data \n",
    "# exploring_snn_architecture/Datasets/rat_pd_sim_data/VariedTrainingData\n",
    "# exploring_snn_architecture/Datasets/rat_pd_sim_data/VariedTrainingData\n",
    "data = build_dataloader(os.path.join(root, training_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of item tensor: torch.Size([32, 150, 80])\n",
      "This represents a batch size of 32, each sample in a batch is over 150 timesteps, and has 80 simulated cells\n",
      "\n",
      "Shape of targets tensor: torch.Size([32, 2])\n",
      "This represents a batch size of 32, each batch has a corresponding target label that is a tuple of size 150 where index 0\n",
      "of that tuple is the alpha-beta oscillation and the index 1 of that tuple is a label 0 (no PD) or 1 (PD)\n"
     ]
    }
   ],
   "source": [
    "data_list = list(data)\n",
    "item, targets = data_list[0]\n",
    "print(f\"Shape of item tensor: {item.shape}\")\n",
    "print(f'This represents a batch size of {len(item)}, each sample in a batch is over {len(item[0])} timesteps, and has {len(item[0][0])} simulated cells\\n')\n",
    "print(f\"Shape of targets tensor: {targets.shape}\")\n",
    "print(f'This represents a batch size of {len(item)}, each batch has a corresponding target label that is a tuple of size {len(item[0])} where index 0')\n",
    "print('of that tuple is the alpha-beta oscillation and the index 1 of that tuple is a label 0 (no PD) or 1 (PD)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_small(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, beta, num_outputs, num_steps, batch_size):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs \n",
    "        self.num_hidden = num_hidden \n",
    "        self.beta = beta \n",
    "        self.num_outputs = num_outputs \n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size \n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        self.fc3 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif3 = snn.Leaky(beta=beta)\n",
    "        self.fc4 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif4 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        spk4_rec = []\n",
    "        mem4_rec = [] \n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[0:self.batch_size, step, 0:self.num_inputs]) \n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "            spk4_rec.append(spk4)\n",
    "            mem4_rec.append(mem4)\n",
    "        \n",
    "\n",
    "        spk1_rec = torch.stack(spk1_rec).detach().cpu() \n",
    "        spk2_rec = torch.stack(spk2_rec).detach().cpu() \n",
    "        spk3_rec = torch.stack(spk3_rec).detach().cpu() \n",
    "        spk4_rec = torch.stack(spk4_rec).detach().cpu() \n",
    "        \n",
    "        return [spk1_rec, spk2_rec, spk3_rec, spk4_rec], mem4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import einsum\n",
    "import snntorch as snn \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "# .memory source: https://github.com/facebookresearch/XLM/blob/main/xlm/model\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def debug(input):\n",
    "    if DEBUG: \n",
    "        print(input)\n",
    "def debug_exit():\n",
    "    assert(DEBUG==False)\n",
    "\n",
    "class Printer(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__() \n",
    "\n",
    "    def forward(self, input): \n",
    "        print(input.size())\n",
    "        return input\n",
    "\n",
    "def exists(v):\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim=-1) * self.scale * self.gamma\n",
    "\n",
    "class ProductKeyMemory(nn.Module):\n",
    "    def __init__(self, dim, num_keys):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_keys = num_keys\n",
    "        self.keys = nn.Parameter(torch.randn(num_keys, dim // 2))\n",
    "        \n",
    "    def forward(self, query):\n",
    "        query = query.view(query.shape[0], 2, -1)\n",
    "        dots = torch.einsum('bkd,nd->bkn', query, self.keys)\n",
    "        return dots.view(query.shape[0], -1)\n",
    "\n",
    "class PEER(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads=8,\n",
    "        num_experts=1_000_000,\n",
    "        num_experts_per_head=16,\n",
    "        activation=nn.GELU,\n",
    "        dim_key=None,\n",
    "        product_key_topk=None,\n",
    "        separate_embed_per_head=False,\n",
    "        pre_rmsnorm=False,\n",
    "        dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()\n",
    "\n",
    "        self.heads = heads\n",
    "        self.separate_embed_per_head = separate_embed_per_head\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        num_expert_sets = heads if separate_embed_per_head else 1\n",
    "\n",
    "        self.weight_down_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "        self.weight_up_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        assert (num_experts ** 0.5).is_integer(), '`num_experts` needs to be a square'\n",
    "        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n",
    "\n",
    "        dim_key = default(dim_key, dim // 2)\n",
    "        self.num_keys = int(num_experts ** 0.5)\n",
    "\n",
    "        self.to_queries = nn.Sequential(\n",
    "            nn.Linear(dim, dim_key * heads * 2, bias=False),\n",
    "            Rearrange('b (p d) -> p b d', p=2)\n",
    "        )\n",
    "\n",
    "        self.product_key_topk = default(product_key_topk, num_experts_per_head)\n",
    "        self.num_experts_per_head = num_experts_per_head\n",
    "\n",
    "        self.keys = nn.Parameter(torch.randn(heads, self.num_keys, 2, dim_key))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, membrain):\n",
    "        x = self.norm(x)\n",
    "        debug(f'x.shape: {x.shape}')\n",
    "        queries = self.to_queries(x)\n",
    "        debug(f'queries.size: {queries.size()}')\n",
    "        debug(f'self.keys.size(): {self.keys.size()}')\n",
    "\n",
    "        sim = einsum(queries, self.keys, 'p b d, h k p d -> p b h k')\n",
    "        debug(f'sim.size() {sim.size()}')\n",
    "        \n",
    "\n",
    "        (scores_x, scores_y), (indices_x, indices_y) = [s.topk(self.product_key_topk, dim=-1) for s in sim]\n",
    "        debug(f'scores_x.size() {scores_x.size()}')\n",
    "        \n",
    "\n",
    "        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n",
    "        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n",
    "        debug(f'all_score.size() {all_scores.size()}')\n",
    "        debug(f'all_indices.size() {all_indices.size()}')\n",
    "\n",
    "\n",
    "        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n",
    "        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n",
    "        debug(f'all_score.size (post view) {all_scores.size()}')\n",
    "        debug(f'all_indices.size (post view) {all_indices.size()}')\n",
    "        debug(all_scores[-1])\n",
    "        \n",
    "\n",
    "        scores, pk_indices = all_scores.topk(self.num_experts_per_head, dim=-1)\n",
    "        debug(f'scores.size {scores.size()}')\n",
    "        debug(f'pk_indices.size {pk_indices.size()}')\n",
    "        indices = all_indices.gather(-1, pk_indices)\n",
    "        debug(f'indices.size {indices.size()}')\n",
    "\n",
    "        debug_exit() \n",
    "\n",
    "        if self.separate_embed_per_head:\n",
    "            head_expert_offsets = torch.arange(self.heads, device=x.device) * self.num_experts\n",
    "            indices = indices + head_expert_offsets.view(1, 1, -1, 1)\n",
    "\n",
    "        weights_down = self.weight_down_embed(pk_indices)\n",
    "        weights_up = self.weight_up_embed(pk_indices)\n",
    "\n",
    "        x = einsum(x, weights_down, 'b d, b n k d -> b n k')\n",
    "\n",
    "        spikes, membrain = self.activation(x) # x is now spike outputs of the LIF neuron, membrain potentials are stored in membrain \n",
    "        x = self.dropout(spikes)\n",
    "\n",
    "        x = x * F.softmax(scores, dim=-1)\n",
    "\n",
    "        x = einsum(x, weights_up, 'b n k, b n k d -> b d')\n",
    "\n",
    "        return x, membrain, spikes\n",
    "    \n",
    "    def init_leaky(self):\n",
    "        return self.activation.init_leaky() \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def forward(self, x):\n",
    "    b, s = x.shape\n",
    "    positions = torch.arange(s, device=x.device).unsqueeze(0).expand(b, s)\n",
    "    \n",
    "    x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "    \n",
    "    for layer in self.layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    x = self.layer_norm(x)\n",
    "    logits = self.lm_head(x)\n",
    "    return logits\n",
    "'''\n",
    "class MOME_net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden, beta, num_outputs, num_steps, batch_size, key_topk=2, per_head_topk=4):\n",
    "        super().__init__()\n",
    "        self.num_steps = num_steps \n",
    "        self.batch_size = batch_size \n",
    "        self.num_inputs = num_inputs \n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.p1 = MOME_layer(num_hidden, num_hidden, beta=beta, key_topk=key_topk, per_head_topk=per_head_topk)\n",
    "        self.p2 = MOME_layer(num_hidden, num_hidden, beta=beta, key_topk=key_topk, per_head_topk=per_head_topk)\n",
    "        # self.p3 = MOME_layer(num_hidden, num_hidden, beta=beta)\n",
    "        # self.p4 = MOME_layer(num_outputs, num_outputs, beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky() \n",
    "        mem2 = self.p1.init_leaky()\n",
    "        mem3 = self.p2.init_leaky()\n",
    "        mem4 = self.lif2.init_leaky() \n",
    "        # mem3 = self.p3.init_leaky()\n",
    "        # mem4 = self.p4.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk1_rec = []\n",
    "        spk2_rec = []\n",
    "        spk3_rec = []\n",
    "        spk4_rec = []\n",
    "        mem4_rec = [] \n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x[0:self.batch_size, step, 0:self.num_inputs]) \n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            spk1_rec.append(spk1)\n",
    "            x1, mem2, spk2 = self.p1(spk1, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            x2, mem3, spk3 = self.p2(x1, mem3)\n",
    "            spk3_rec.append(spk3)\n",
    "            cur2 = self.fc2(x2)\n",
    "            spk4, mem4 = self.lif2(cur2, mem4)\n",
    "            spk4_rec.append(spk4)\n",
    "            mem4_rec.append(mem4)\n",
    "        \n",
    "\n",
    "        spk1_rec = torch.stack(spk1_rec).detach().cpu() \n",
    "        spk2_rec = torch.stack(spk2_rec).detach().cpu() \n",
    "        spk3_rec = torch.stack(spk3_rec).detach().cpu() \n",
    "        spk4_rec = torch.stack(spk4_rec).detach().cpu() \n",
    "        \n",
    "        return [spk1_rec, spk2_rec, spk3_rec, spk4_rec], mem4\n",
    "    \n",
    "\n",
    "\n",
    "# https://github.com/facebookresearch/XLM/blob/main/xlm/model/transformer.py \n",
    "# top k selection involves generating a k value for each expert, that allows evaluation of which expert is best for a specific task \n",
    "# this is the most complex portion of a mixture of experts model \n",
    "# Each MOME layer must make a selection of experts each time \n",
    "'''\n",
    "        self,\n",
    "        dim, -> num inputs \n",
    "        *,\n",
    "        heads=8,\n",
    "        num_experts=1_000_000,-> also num inputs? \n",
    "        num_experts_per_head=16, -> other num outputs \n",
    "        activation=nn.GELU,\n",
    "        dim_key=None,\n",
    "        product_key_topk=None, -> num outputs \n",
    "        separate_embed_per_head=False,\n",
    "        pre_rmsnorm=False,\n",
    "        dropout=0.\n",
    "'''\n",
    "class MOME_layer(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_outputs: int, beta, key_topk=2, per_head_topk=4): \n",
    "        super().__init__()\n",
    "        self.num_experts = num_inputs\n",
    "\n",
    "        self.peer = PEER(num_inputs, heads=1, num_experts=self.num_experts, product_key_topk=key_topk, activation=snn.Leaky(beta=beta), num_experts_per_head=per_head_topk)\n",
    "\n",
    "    def forward(self, data, mem):\n",
    "        data, mem, spk = self.peer(data, mem)\n",
    "        \n",
    "        return data, mem, spk\n",
    "    \n",
    "    def init_leaky(self):\n",
    "        return self.peer.init_leaky() \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_spike(spikes): \n",
    "    output = []\n",
    "    unpacked = []\n",
    "    for item in spikes: \n",
    "        # time * batch * neurons \n",
    "        # sum in time and neuron dimensions \n",
    "        unpacked.append(item.sum(0).sum(-1))\n",
    "        #gives spike count by batch \n",
    "        #print(item.sum(0).sum(-1).size())\n",
    "    for i in range(unpacked[0].size()[0]): \n",
    "        count = 0\n",
    "        for item in unpacked:\n",
    "            count += torch.tensor(item[i].item()) \n",
    "        \n",
    "        output.append(count)\n",
    "    return torch.stack(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(data, model, num_epochs, loss, optimizer, scheduler):\n",
    "    loss_hist = []\n",
    "    dtype = torch.float\n",
    "    loss_by_epoch = [] \n",
    "    spk_hist = [] \n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = [] \n",
    "        #iter_counter = 0\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for i, (item, targets) in enumerate(iter(data)):\n",
    "            targets = targets[0:batch_size, 1:2]\n",
    "            targets = targets.flatten()\n",
    "\n",
    "            targets = targets.type(torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            model.train()\n",
    "            spk_rec, mem_rec = model(item)\n",
    "            spk_hist.append(spk_rec) \n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "            loss_val = torch.sum(loss(mem_rec, targets))\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "            epoch_loss.append(loss_val.item())\n",
    "\n",
    "            # Calculate accuracy without SF.accuracy_rate\n",
    "            pred = torch.argmax(mem_rec.cpu().detach(), dim=1)\n",
    "            accuracy = metrics.accuracy_score(targets.cpu().detach(), pred.cpu().detach())\n",
    "\n",
    "            # Print accuracy\n",
    "            if loss_val.item() > 0.001:\n",
    "                if i % 2 == 0:\n",
    "                    print(f\"Epoch {epoch}, Iteration {i}, Train Loss: {loss_val.item():.2f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "            else:\n",
    "                if i % 20 == 0:\n",
    "                    print(f\"Epoch {epoch}, Iteration {i}, Train Loss: {loss_val.item():.2f}, Accuracy: {accuracy * 100:.2f}%\")\n",
    "        if scheduler is not None: \n",
    "            scheduler.step() \n",
    "\n",
    "        loss_by_epoch.append(epoch_loss)\n",
    "    \n",
    "    return (loss_hist, loss_by_epoch, spk_hist)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Train Loss: 0.93, Accuracy: 46.88%\n",
      "Epoch 0, Iteration 2, Train Loss: 0.96, Accuracy: 34.38%\n",
      "Epoch 0, Iteration 4, Train Loss: 0.79, Accuracy: 40.62%\n",
      "Epoch 0, Iteration 6, Train Loss: 0.94, Accuracy: 37.50%\n",
      "Epoch 0, Iteration 8, Train Loss: 0.69, Accuracy: 53.12%\n",
      "Epoch 0, Iteration 10, Train Loss: 0.56, Accuracy: 71.88%\n",
      "Epoch 0, Iteration 12, Train Loss: 0.60, Accuracy: 68.75%\n",
      "Epoch 0, Iteration 14, Train Loss: 0.56, Accuracy: 71.88%\n",
      "Epoch 0, Iteration 16, Train Loss: 0.53, Accuracy: 71.88%\n",
      "Epoch 0, Iteration 18, Train Loss: 0.63, Accuracy: 62.50%\n",
      "Epoch 0, Iteration 20, Train Loss: 0.42, Accuracy: 84.38%\n",
      "Epoch 0, Iteration 22, Train Loss: 0.35, Accuracy: 90.62%\n",
      "Epoch 0, Iteration 24, Train Loss: 0.32, Accuracy: 96.88%\n",
      "Epoch 0, Iteration 26, Train Loss: 0.29, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 28, Train Loss: 0.31, Accuracy: 93.75%\n",
      "Epoch 0, Iteration 30, Train Loss: 0.20, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 0, Train Loss: 0.17, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 2, Train Loss: 0.16, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 4, Train Loss: 0.10, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 6, Train Loss: 0.13, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 8, Train Loss: 0.13, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 10, Train Loss: 0.09, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 12, Train Loss: 0.08, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 14, Train Loss: 0.13, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 16, Train Loss: 0.10, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 18, Train Loss: 0.10, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 20, Train Loss: 0.09, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 22, Train Loss: 0.10, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 24, Train Loss: 0.11, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 26, Train Loss: 0.07, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 28, Train Loss: 0.08, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 30, Train Loss: 0.06, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 0, Train Loss: 0.06, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 2, Train Loss: 0.07, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 4, Train Loss: 0.09, Accuracy: 96.88%\n",
      "Epoch 2, Iteration 6, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 8, Train Loss: 0.06, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 10, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 12, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 14, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 16, Train Loss: 0.06, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 18, Train Loss: 0.06, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 20, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 22, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 24, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 26, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 28, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 30, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 0, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 2, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 4, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 6, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 8, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 10, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 12, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 16, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 18, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 20, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 22, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 24, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 28, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 30, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 0, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 2, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 4, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 6, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 8, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 10, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 12, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 16, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 18, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 20, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 22, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 24, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 28, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 30, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 0, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 2, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 4, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 6, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 8, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 10, Train Loss: 0.04, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 12, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 16, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 18, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 20, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 22, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 24, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 28, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 30, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 0, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 2, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 4, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 6, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 8, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 10, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 12, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 16, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 18, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 20, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 22, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 24, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 28, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 30, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 0, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 2, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 4, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 6, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 8, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 10, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 12, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 16, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 18, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 20, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 22, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 24, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 28, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 30, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 0, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 2, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 4, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 6, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 8, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 10, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 12, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 14, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 16, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 18, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 20, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 22, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 24, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 26, Train Loss: 0.03, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 28, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 30, Train Loss: 0.01, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 0, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 2, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 4, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 6, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 8, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 10, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 12, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 14, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 16, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 18, Train Loss: 0.01, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 20, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 22, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 24, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 26, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 28, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 30, Train Loss: 0.02, Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MOME_net(\n",
       "  (fc1): Linear(in_features=80, out_features=144, bias=True)\n",
       "  (lif1): Leaky()\n",
       "  (p1): MOME_layer(\n",
       "    (peer): PEER(\n",
       "      (norm): Identity()\n",
       "      (weight_down_embed): Embedding(144, 144)\n",
       "      (weight_up_embed): Embedding(144, 144)\n",
       "      (activation): Leaky()\n",
       "      (to_queries): Sequential(\n",
       "        (0): Linear(in_features=144, out_features=144, bias=False)\n",
       "        (1): Rearrange('b (p d) -> p b d', p=2)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (p2): MOME_layer(\n",
       "    (peer): PEER(\n",
       "      (norm): Identity()\n",
       "      (weight_down_embed): Embedding(144, 144)\n",
       "      (weight_up_embed): Embedding(144, 144)\n",
       "      (activation): Leaky()\n",
       "      (to_queries): Sequential(\n",
       "        (0): Linear(in_features=144, out_features=144, bias=False)\n",
       "        (1): Rearrange('b (p d) -> p b d', p=2)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc2): Linear(in_features=144, out_features=2, bias=True)\n",
       "  (lif2): Leaky()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mome_net = MOME_net(num_inputs=num_inputs, num_hidden=num_hidden, beta=beta, num_outputs=num_outputs, num_steps=num_steps, batch_size=batch_size, key_topk=key_topk, per_head_topk=per_head_topk).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mome_net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = None\n",
    "if lr_decay: \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, lr_decay_gamma)\n",
    "loss_hist_mome, loss_by_epoch_mome, spk_hist = train_loop(data, mome_net, num_epochs, loss, optimizer, scheduler)\n",
    "mome_net.cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons for x: 244\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "\n",
    "if normalize_params: \n",
    "    params_mome = count_parameters(mome_net)\n",
    "    #x = hidden_neurons_normalized_for_param_count\n",
    "    #h = hidden_layers \n",
    "    #o = num_outputs\n",
    "    #n = num_inputs\n",
    "    #y = params_mome \n",
    "    # y == (n * x + x) + ((h - 1) * (x * x + x)) + (x * o + o)  \n",
    "    \n",
    "    # Define variables\n",
    "    x, o, n, h, y = symbols('x o n h y')\n",
    "    \n",
    "    # Define the equation\n",
    "    equation = Eq((n * x + x) + ((h - 1) * (x * x + x)) + (x * o + o), y)\n",
    "    \n",
    "    # Solve for x\n",
    "    solution = solve(equation.subs({o: num_outputs, n: num_inputs, h: hidden_layers, y: params_mome}), x)\n",
    "    real_solutions = [sol.evalf() for sol in solution if sol.is_real]\n",
    "    hidden_neurons_normalized_for_param_count = round(max(real_solutions)) \n",
    "    print(\"Neurons for x:\", hidden_neurons_normalized_for_param_count)\n",
    "    \n",
    "    # holy fuck this was annoying math omg \n",
    "    num_hidden_standard = hidden_neurons_normalized_for_param_count\n",
    "else: \n",
    "    num_hidden_standard = num_hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Train Loss: 1.10, Accuracy: 43.75%\n",
      "Epoch 0, Iteration 2, Train Loss: 0.65, Accuracy: 59.38%\n",
      "Epoch 0, Iteration 4, Train Loss: 0.59, Accuracy: 84.38%\n",
      "Epoch 0, Iteration 6, Train Loss: 0.40, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 8, Train Loss: 0.24, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 10, Train Loss: 0.12, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 12, Train Loss: 0.05, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 14, Train Loss: 0.01, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 16, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 22, Train Loss: 0.05, Accuracy: 96.88%\n",
      "Epoch 0, Iteration 26, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 0, Iteration 28, Train Loss: 0.27, Accuracy: 90.62%\n",
      "Epoch 1, Iteration 0, Train Loss: 0.02, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 2, Train Loss: 0.21, Accuracy: 96.88%\n",
      "Epoch 1, Iteration 14, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 1, Iteration 28, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 2, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 3, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 4, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 5, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 6, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 7, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 8, Iteration 20, Train Loss: 0.00, Accuracy: 100.00%\n",
      "Epoch 9, Iteration 0, Train Loss: 0.00, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "net = Net_small(num_inputs=num_inputs, num_hidden=num_hidden_standard, beta=beta, num_outputs=num_outputs, num_steps=num_steps, batch_size=batch_size).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = None\n",
    "if lr_decay: \n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, lr_decay_gamma)\n",
    "loss_hist, loss_by_epoch, spk_hist_base = train_loop(data, net, num_epochs, loss, optimizer, scheduler)\n",
    "net.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    \"beta\": beta,\n",
    "    \"learning_rate\": lr,\n",
    "    \"learning_rate_decay\": lr_decay,\n",
    "    \"learning_rate_decay_gamma\": lr_decay_gamma, \n",
    "    \"batch_size\": batch_size, \n",
    "    \"num_epochs\": num_epochs, \n",
    "    \"hidden_width_mome\": num_hidden,\n",
    "    \"hidden_width_standard\": int(num_hidden_standard),\n",
    "    \"params_mome\": count_parameters(mome_net), \n",
    "    \"params_standard\": count_parameters(net), \n",
    "    \"hidden_layers\": hidden_layers,\n",
    "    \"key_topk\": key_topk, \n",
    "    \"per_head_topk\": per_head_topk, \n",
    "    \"normalize_params\": normalize_params,\n",
    "    \"normalize_neurons\": normalize_neurons, \n",
    "    \"training_data\": training_data, \n",
    "    \"testing_data\": testing_data, \n",
    "}\n",
    "print(params_dict) \n",
    "write_params(os.path.join(out_folder_name, \"run_params.json\"), params_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "\n",
    "\n",
    "mome = [] \n",
    "for item in loss_by_epoch_mome: \n",
    "    mome.append(statistics.mean(item))\n",
    "    \n",
    "standard = [] \n",
    "for item in loss_by_epoch: \n",
    "    standard.append(statistics.mean(item))\n",
    "    \n",
    "X = range(len(standard)) \n",
    "\n",
    "X_axis = np.arange(len(X)) \n",
    "  \n",
    "plt.bar(X_axis - 0.2, mome, 0.4, label = 'MOME', color=\"blue\") \n",
    "plt.bar(X_axis + 0.2, standard, 0.4, label = 'Standard', color=\"red\") \n",
    "  \n",
    "plt.xticks(X_axis, X) \n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average Loss')\n",
    "ax.set_title('Average Loss by Epoch')\n",
    "ax.set_ylim((0, .8))\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Average_Loss_by_Epoch.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss over time\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(len(loss_hist_mome)), loss_hist_mome, c=\"red\", label=\"MOME_snn\")\n",
    "ax.plot(range(len(loss_hist)), loss_hist, c=\"blue\", label=\"standard_snn\")\n",
    "ax.set_ylim((0,.8))\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Over Time')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Loss_Over_Time.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_outliers(data, m = 2.):\n",
    "    d = np.abs(data - np.median(data))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else np.zeros(len(d))\n",
    "    return data[s<m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(reject_outliers(np.array(loss_hist)))\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve \n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = range(len(loss_hist_mome))\n",
    "\n",
    "z = np.polyfit(x, loss_hist_mome, 20)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(x, loss_hist_mome, p(x))\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curve with Fitted Polynomial MOME')\n",
    "ax.set_ylim((0, .8))\n",
    "\n",
    "plt.savefig(os.path.join(out_folder_name, \"Loss_Curve_with_Fitted_Polynomial_MOME.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss curve \n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = range(len(loss_hist))\n",
    "\n",
    "z = np.polyfit(x, loss_hist, 20)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(x, loss_hist, p(x))\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curve with Fitted Polynomial Standard')\n",
    "ax.set_ylim((0, .8))\n",
    "\n",
    "plt.savefig(os.path.join(out_folder_name, \"Loss_Curve_with_Fitted_Polynomial_Standard.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch_accuracy(data, targets, net, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    if train:\n",
    "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(data_eval, model):\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "    preds = [] \n",
    "    spike_counts = []\n",
    "    net.eval()\n",
    "    for item, targets in iter(data_eval):\n",
    "        targets = targets[0:batch_size, 1:2]\n",
    "        targets = targets.flatten()\n",
    "        targets = targets.type(torch.long)\n",
    "\n",
    "        #Get predictions\n",
    "        spk_rec, mem_rec = model(item)\n",
    "        spike_counts = spike_counts + count_spike(spk_rec).tolist()\n",
    "\n",
    "        #Convert to binary \n",
    "        pred = torch.argmax(mem_rec.detach(), dim=1)\n",
    "        # Calculate metrics \n",
    "\n",
    "\n",
    "\n",
    "        f1_score.append(metrics.f1_score(targets.detach().cpu(), pred.cpu()))\n",
    "        accuracy.append(metrics.accuracy_score(targets.detach().cpu(), pred.cpu()))\n",
    "        preds.extend(pred.tolist())\n",
    "\n",
    "    net.train()\n",
    "    return(spike_counts, preds, f1_score, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = num_steps_eval \n",
    "eval_set = build_dataloader(os.path.join(root, testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(device) \n",
    "spike_count, model_predictions, f1_score, accuracy = eval_loop(eval_set, net)\n",
    "net.cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mome_net.to(device) \n",
    "spike_count_mome, model_predictions_mome, f1_score_mome, accuracy_mome = eval_loop(eval_set, mome_net)\n",
    "mome_net.cpu() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(len(spike_count)), spike_count, color=\"blue\", label=\"standard\")\n",
    "ax.plot(range(len(spike_count_mome)), spike_count_mome, color=\"red\", label=\"MOME\")\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Data Point')\n",
    "ax.set_ylabel('Spike Count')\n",
    "ax.set_title('Spike Count by Data Point')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Spike_Count_by_Data_Point.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_spike_counts = []\n",
    "pd_indexes = [] \n",
    "con_spike_counts = []\n",
    "con_indexes = []  \n",
    "count = 0 \n",
    "\n",
    "\n",
    "for i in range(len(spike_count)): \n",
    "    if model_predictions[i] == 1: \n",
    "        pd_spike_counts.append(spike_count[i])\n",
    "        pd_indexes.append(count) \n",
    "    else: \n",
    "        con_spike_counts.append(spike_count[i])\n",
    "        con_indexes.append(count)\n",
    "    count += 1 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "#plt.setp(ax, xticks=range(max(len(no_pd_labels), len(pd_labels))))\n",
    "ax.scatter(pd_indexes, pd_spike_counts, color='red', label='PD')\n",
    "ax.scatter(con_indexes, con_spike_counts, color='blue', label='No PD')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Total Spikes')\n",
    "ax.set_title('Comparison of Total Spike Counts in Model Between Samples With and Without PD')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Spike_Counts_in_Model_Between_Samples_With_and_Without_PD_standard.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_spike_counts = []\n",
    "pd_indexes = [] \n",
    "con_spike_counts = []\n",
    "con_indexes = []  \n",
    "count = 0 \n",
    "\n",
    "\n",
    "for i in range(len(spike_count_mome)): \n",
    "    if model_predictions_mome[i] == 1: \n",
    "        pd_spike_counts.append(spike_count_mome[i])\n",
    "        pd_indexes.append(count) \n",
    "    else: \n",
    "        con_spike_counts.append(spike_count_mome[i])\n",
    "        con_indexes.append(count)\n",
    "    count += 1 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "#plt.setp(ax, xticks=range(max(len(no_pd_labels), len(pd_labels))))\n",
    "ax.scatter(pd_indexes, pd_spike_counts, color='red', label='PD')\n",
    "ax.scatter(con_indexes, con_spike_counts, color='blue', label='No PD')\n",
    "\n",
    "ax.set_xlabel('Sample Index')\n",
    "ax.set_ylabel('Total Spikes')\n",
    "ax.set_title('Comparison of Total Spike Counts in Model Between Samples With and Without PD MOME')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Spike_Counts_in_Model_Between_Samples_With_and_Without_PD_mome.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "spike_count_average_by_epoch = [np.mean(spike_count[i:i+batch_size]) for i in range(0,len(spike_count),batch_size)]\n",
    "ax.plot(range(len(spike_count_average_by_epoch)), spike_count_average_by_epoch, color=\"blue\", label=\"standard\")\n",
    "spike_count_average_by_epoch_mome = [np.mean(spike_count_mome[i:i+batch_size]) for i in range(0,len(spike_count_mome),batch_size)]\n",
    "ax.plot(range(len(spike_count_average_by_epoch_mome)), spike_count_average_by_epoch_mome, color=\"red\", label=\"MOME\")\n",
    "# Adding labels\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('Spike Count')\n",
    "ax.set_title('Mean Spike Count by Batch')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Mean_Spike_Count_by_Batch.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(spike_count_average_by_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mome = np.mean(spike_count_average_by_epoch_mome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_improvement = (mean - mean_mome) / mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot f1 \n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(len(f1_score)), f1_score, color=\"blue\", label=\"standard\")\n",
    "ax.plot(range(len(f1_score_mome)), f1_score_mome, color=\"red\", label=\"MOME\")\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score by Batch')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"f1_by_Batch.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(len(accuracy)), accuracy, color=\"blue\", label=\"standard\")\n",
    "ax.plot(range(len(accuracy_mome)), accuracy_mome, color=\"red\", label=\"MOME\")\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by Batch')\n",
    "ax.legend()\n",
    "plt.savefig(os.path.join(out_folder_name, \"Accuracy_by_Batch.png\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
